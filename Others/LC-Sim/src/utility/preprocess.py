#!/usr/bin/python
#coding:utf-8
'''The individual preprocess module, which processes original
trace file, and prepare data for the sim.py.
@version: 1.0
@author: U{Baohua Yang<mailto:yangbaohua@gmail.com>}
@created: Oct 12, 2011
@last update: Oct 18, 2011
@see: U{<https://github.com/yeasy/lazyctrl>}
@TODO: nothing
'''
import csv, os, math, networkx
import glob, fileinput, random
from optparse import OptionParser

def extractRecordFromTrace(fn_in, fn_out):
    """ Simply extract the 'src dst octs' from each record in the original trace file, 
    and save the results with formatting 'host1 host2 octs' to out file, all id starts from 1.
    @param fn_in: Name of the input orginal trace file: *.csv
    @param fn_out: Name of the output file: .record.txt
        format: src dst oct_of_the_flow
    """
    if not fn_in or not fn_out:
        return
    f_in = open(fn_in, "r")
    f_out = open(fn_out, "a")
    i,src, dst, octs =0, 0, 0, 0
    try:
        data = csv.reader(f_in, delimiter=",")
        for line in data:
            i += 1
            src, dst, octs = int(line[14]), int(line[15]), int(line[13]) #O,P,N
            src += 1
            dst += 1
            f_out.write("%u %u %u\n" % (src, dst, octs))
    finally:
        f_in.close()
        f_out.close()
        print "Extract %d records from original trace." %i
    
def aggregateRecord(fn_in, fn_out):
    """
    Read the trace record generated by extractRecordFromTrace, 
    and aggregate them into un-directional host pairs, and re-assign host id
    after removing unrecorded hosts here.
    @param fn_in: Name of the input traffic file with each line meaning one flow.
        format: src dst octs
    @param fn_out: Name of the output file: .agg.txt
        format: src dst flows octs
    """
    if not fn_in or not fn_out:
        return
    print fn_in
    f_out = open(fn_out, "w")
    src, dst, octs = 0, 0, 0
    trafficDic = {} #{(host1, host2):[flow, traffic]}
    i,hid,t = 0,1,()
    #hidDic = {}  #{host: hostid}
    hidSet=set()
    try:
        for line in fileinput.input([fn_in]):
            i += 1
            if i % 1000000 == 0:
                print "i=", i
            src, dst, octs = line.split()
            src, dst, octs = int(src), int(dst), int(octs)
            hidSet.add(src)
            hidSet.add(dst)
            t = tuple(sorted([src, dst]))
            if t not in trafficDic:
                trafficDic[t] = [1,octs]
            else:
                trafficDic[t][0] += 1
                trafficDic[t][1] += octs
        for k in trafficDic:
            f_out.write("%u %u %u %u\n" % (k[0], k[1], trafficDic[k][0], trafficDic[k][1]))
        print " %d records are read, with %d hosts [%d,%d], flows=%d, octs=%d"\
                %(i,len(hidSet),min(hidSet),max(hidSet),sum([i[0] for i in trafficDic.values()]),sum([i[1] for i in trafficDic.values()]))
    finally:
        f_out.close()

def getFocus(fn_in="sw_affinity.txt"):
    """
    Calculate the focus value for each host.
    F_i=max(V_i,j)/V_i
    @param fn_in: Name of the aggregated flow file.
    """
    if not fn_in:
        return
    src, dst, flow, oct = 0, 0, 0, 0
    f_in = open(fn_in, "r")
    i, weight = 0,0
    edges, focusDic = [], {}
    G = networkx.Graph()
    #num_nodes,num_edges = 0,0
    try:
        #num_nodes = int(f_in.readline())
        #num_edges =  int(f_in.readline())
        #print "Import begin, nodes=%d, edges=%d" %(num_nodes,num_edges)
        for line in f_in.readlines():
            i += 1
            if i %10000 == 0:
                print "read in %d lines." %i
            src, dst, flow, oct = line.split()
            src, dst, flow = int(src), int(dst), int(flow)
            if (src,dst) in edges or (dst,src) in edges:
                print "WARN, already in edges."
            weight = flow
            edges.append((src, dst, weight))
    finally:
        G.add_weighted_edges_from(edges)
        print "Import done, nodes=%d, edges=%d" %(len(G.nodes()),len(G.edges()))
        for u in G.nodes():
            weight_list = [G.edge[u][v]['weight'] for v in G.neighbors(u)]
            focusDic[u] = float(max(weight_list))/sum(weight_list)
        #print focusDic
        print "focus: avg=%f, max=%f" %(sum(focusDic.values())/len(focusDic.values()),max(focusDic.values()))

def genRandomAgg(num_host, fn_out):
    """
    Generate randomly traffic aggregated matrix, host_id is in [1,num_host].
    @param num_host: Name of the hosts
    @param fn_out: Name of the output file: .random.agg.txt
        format: src dst flows octs(1 defaultly)
    """
    if not fn_out:
        return
    f_out = open(fn_out, "w")
    src, dst, flow, oct = 0, 0, 0, 1
    t = ()
    #hidDic = {}  #{host: hostid}
    hostPairSet=set()
    num_flow = min([100000, num_host*5])
    max_flow_weight = min([10000,num_flow])
    try:
        for i in xrange(num_flow):
            if (random.randint(1,10)<=9):
                src, dst = random.randint(1,num_host/10),random.randint(1,num_host/10)
                flow = random.randint(max_flow_weight/10,max_flow_weight)
            else:
                src, dst = random.randint(num_host/10,num_host),random.randint(num_host/10,num_host)
                flow = random.randint(1,max_flow_weight/10)
            t = tuple(sorted([src, dst]))
            while src == dst or t in hostPairSet:
                if (random.randint(1,10)<=9):
                    src, dst = random.randint(1,num_host/10),random.randint(1,num_host/10)
                    flow = random.randint(max_flow_weight/10,max_flow_weight)
                else:
                    src, dst = random.randint(num_host/10,num_host),random.randint(num_host/10,num_host)
                    flow = random.randint(1,max_flow_weight/10)
                t = tuple(sorted([src, dst]))
            hostPairSet.add(t)
            f_out.write("%u %u %u %u\n" % (src, dst, flow, oct))
        print " %d flows are generated, with %d hosts" %(num_flow,num_host)
    finally:
        f_out.close()

def getFlowId(max_id):
    """
    The function will return a flow id.
    The return flow id will match the probability function of original trace,
    with occurences as weight. 
    """
    randNum = random.randint(1,100)
    if randNum <= 33: #flow 1
        flowid = 0
    elif randNum <= 40:
        flowid = 1
    elif randNum <= 45:
        flowid = 2
    elif randNum <= 50:
        flowid = 3
    elif randNum <= 54:
        flowid = 4
    elif randNum <= 58:
        flowid = 5
    elif randNum <= 61:
        flowid = 6
    elif randNum <= 65:
        flowid = 7
    elif randNum <= 68:
        flowid = 8
    elif randNum <= 70:
        flowid = 9
    elif randNum <= 73:
        flowid = 10
    elif randNum <= 82:
        flowid = 11 + (82-74)/(18-11)
    elif randNum <= 84:
        flowid = random.randint(19,22)
    elif randNum <= 85:
        flowid = random.randint(23,26)
    elif randNum <= 86:
        flowid = random.randint(27,33)
    elif randNum <= 90:
        flowid = random.randint(34,57)
    elif randNum <= 96:
        flowid = [random.randint(58,95),random.randint(96,162),random.randint(163,309)][(randNum-91)/2]
    else:
        flowid = [random.randint(310,444),random.randint(444,642),random.randint(444,1000),random.randint(1001,max_id)][randNum-97]
    return flowid

def scaleData(fn_in="host.seed.agg.txt", fn_out="host.scale.agg.txt",scale=10):
    """ read the host_flow file generated by aggregateRecord, put each host into sw randomly.
        generate the flow matrix between every SW pairs, and the host-sw mapping information.
    @param fn_in: Name of the input aggregated traffic record file
        format: host1 host2 flow oct
    """
    if not fn_in or not fn_out:
        return
    src, dst, flow, oct = 0,0,0,0.0
    src_sw, dst_sw = 0,0
    tList = [] #[(host1,host2,flow,oct),(host1,host2,flow,oct),...]
    hostSet= set()
    try:
        f_out = open(fn_out, "w")
        for line in fileinput.input([fn_in]): #read in the original file
            src, dst, flow, oct = line.split() #oct is not used here
            src, dst, flow, oct = int(src), int(dst), int(flow), int(oct)
            if src == dst:
                continue
            hostSet.add(src)
            hostSet.add(dst)
            tList.append((src, dst, flow, oct))
        tList.sort(reverse=True,key=lambda x:x[2]) #sort descending on flow
        #print "Top several flow pairs"
        #print tList[0],tList[1],tList[2],tList[3]

        #write scaled data into out file
        cluster_size = len(hostSet) #size of each cluster
        cluster_flows = 5000  #num of flows in each generated cluster
        print "Begin to generate with scale=%d, cluster_size=%d, cluster_flow=%d" %(scale,cluster_size,cluster_flows)
        hostSet=set()
        for i in range(scale):
            idSet=set()
            for k in xrange(cluster_flows): #each cluster
                id = getFlowId(len(tList)-1)
                while id in idSet:
                    id = getFlowId(len(tList)-1)
                idSet.add(id)
                flow=tList[id][2]
                oct=tList[id][3]
                f_out.write("%u %u %u %u\n" % (i*cluster_size+tList[id][0],i*cluster_size+tList[id][1],flow,oct))
                hostSet.add(i*cluster_size+tList[id][0])
                hostSet.add(i*cluster_size+tList[id][1])
                if random.randint(1,100)<=50:
                    f_out.write("%u %u %u %u\n" % (i*cluster_size+tList[id][0],((i+1)%scale)*cluster_size+tList[id][1],flow,oct))
    finally:
        print "Scale done, with max_host_id=%d" %(max(hostSet))
        f_out.close()

NUM_SW_PORT = 24

def genSWAffinity(fn_in="host_flow_oct.agg.txt", fn_affinity="sw_affinity.txt",fn_map="host_sw_map.txt",num_host=6509):
    """ read the host_flow file generated by aggregateRecord, put each host into sw randomly.
        generate the flow matrix between every SW pairs, and the host-sw mapping information.
    @param fn_in: Name of the input aggregated traffic record file
    @param fn_affinity: Name of the output sw affinity file
        format:
            num_of_sw
            num_of_record
            sw1 sw2 affinity
            ...
    @param fn_map: Name of the host-sw mapping file 
    """
    if not fn_in or not fn_affinity:
        return
    NUM_SW = int(math.ceil(num_host/float(NUM_SW_PORT)))
    f_out = open(fn_affinity, "w")
    f_map = open(fn_map, "w")
    src, dst, flow, oct = 0,0,0,0.0
    src_sw, dst_sw = 0,0
    hostDic, swDic = {}, {} #{host:sw},{(sw1,sw2):flow}
    random.seed()
    try:
        for line in fileinput.input([fn_in]):
            src, dst, flow, oct = line.split() #oct is not used here
            src, dst, flow = int(src), int(dst), int(flow)
            if src == dst:
                continue
            if src not in hostDic: #generate the sw_id of src host, starting with 1
                src_sw = src%NUM_SW + 1
                #src_sw = int(math.ceil(src/float(NUM_SW_PORT)))
                #src_sw = random.randint(1,NUM_SW)
                hostDic[src] = src_sw
            else:
                src_sw = hostDic[src]
            if dst not in hostDic: #generate the sw_id of dst host, starting with 1
                dst_sw = dst%NUM_SW + 1
                #dst_sw = int(math.ceil(dst/float(NUM_SW_PORT)))
                #dst_sw = random.randint(1,NUM_SW)
                hostDic[dst] = dst_sw
            else:
                dst_sw = hostDic[dst]
            if src_sw == dst_sw: #intra-switch flows are not needed for affinity
                continue
            t = tuple(sorted([src_sw, dst_sw]))
            if t not in swDic:
                swDic[t] = flow
            else:
                swDic[t] += flow
        #write host-sw mapping
        for k in hostDic:
            f_map.write("%u %u\n" % (k, hostDic[k]))
        #write out affinity file
        print "#sw=%u, #edges=%u" %(len(set(hostDic.values())),len(swDic))
        f_out.write("%u\n" %len(set(hostDic.values()))) #number of nodes
        f_out.write("%u\n" %len(swDic)) #number of flow records/edges
        sum=0
        for k in swDic:
            f_out.write("%u %u %u\n" % (k[0], k[1], swDic[k]))
            sum += swDic[k]
        print "sum_inter_sw_flows=",sum
    finally:
        f_out.close()
        f_map.close()

    
if __name__ == "__main__":
    parser = OptionParser(usage="Usage: %prog [-i FILE][-o FILE][-n NUM][-m EXTRACT|AGG|AFFINITY|FOCUS|SCALE|RANDOM]", version="%prog 1.0")
    parser.add_option("-i", "--input_file",
            action="store", type="string",
            dest="input", default="host_pair_flow.txt",
            help="read trace data from FILE", metavar="FILE")
    parser.add_option("-o", "--output_file",
            action="store", type="string",
            dest="output", default="",
            help="write result into FILE", metavar="FILE")
    parser.add_option("-n", "--num",
            action="store", type="int",
            dest="num", default=0,
            help="number of hosts")
    parser.add_option("-m", "--mode",
            action="store", type="string",
            dest="mode", default="",
            help="indicate the mode, \
                      EXTRACT will extract record from the original trace file into (host1 host2 oct), should input .csv file or directory path containing original .csv trace files; \
                      AGG will aggregate the trace file into (host1 host2 flow oct), should input record file; \
                      AFFINITY will further generate the sw_affnity file, and the host-sw mapping file, should input aggregated record file; \
                      FOCUS will generate the focus value fromt the agg file: F_i=max(V_i,j)/V_i; \
                      SCALE will scale the .agg file with given scale; \
                      RANDOM will generate random .agg file with given num_host", \
                      metavar="MODE")
    (opt, args) = parser.parse_args()
    if not opt.input or not opt.mode or (opt.mode=="AFFINITY" and opt.num==0):
        print "Invalid parameter number, try -h"
        print "Usage: %prog [-i FILE][-o FILE][-n NUM][-m EXTRACT|AGG|AFFINITY|FOCUS|SCALE|RANDOM]"
        exit()
    if opt.mode.upper() == 'EXTRACT': #extract record from the original trace file
        if os.path.isdir(opt.input): #given a directory
            for fn in glob.glob(opt.input + os.sep + '*.csv'):
                extractRecordFromTrace(fn, fn.replace(".csv",".record.txt"))
        else:
            extractRecordFromTrace(opt.input, opt.output)
    elif opt.mode.upper() == 'AGG': #aggregate the (host1 host2 flow oct) file
        if os.path.isdir(opt.input): #given a directory
            for fn in glob.glob(opt.input + os.sep + '*.txt'):
                aggregateRecord(fn, fn.replace(".record.txt",".agg.txt"))
        else:
            aggregateRecord(opt.input, opt.output)
    elif opt.mode.upper() == 'SCALE': #scale the .agg file
        if not opt.output:
            opt.output = opt.input.replace(".agg.txt","_scale.agg.txt")
        scaleData(opt.input, opt.output, 10)
    elif opt.mode.upper() == 'FOCUS': #scale the .agg file
            getFocus(opt.input)
    elif opt.mode.upper() == 'AFFINITY': #calculate the sw_affnity file, and the host-sw mapping file 
        if os.path.isdir(opt.input): #given a directory
            for fn in glob.glob(opt.input + os.sep + '*.agg.txt'):
                genSWAffinity(fn, fn.replace(".agg.txt",".sw_affinity.txt"), fn.replace(".agg.txt",".host_sw_map.txt"),opt.num)
        else:
            genSWAffinity(opt.input, opt.output, opt.input.replace(".agg.txt",".host_sw_map.txt"),opt.num)
    elif opt.mode.upper() == 'RANDOM': #generate randomly .random.agg.txt file
        if opt.num <= 0:
            opt.num = 100000
        if not opt.output:
            opt.output = "test_%dhosts.random.agg.txt" %opt.num
        genRandomAgg(opt.num, opt.output)
